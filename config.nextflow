// nextflow.config - SLURM configuration

// Enable SLURM executor
process.executor = 'slurm'

// Global process settings
process {
    // Default resource requirements
    cpus = 1
    memory = '4 GB'
    time = '1h'
    
    // Default SLURM queue/partition
    queue = 'short-cpu'
    
    // SLURM job options
    clusterOptions = '--nodes=1'
    
    // Process-specific resource requirements
    withName: 'SPLIT_FASTQ' {
        cpus = 2
        memory = '8 GB'
        time = '30m'
        queue = 'day-long-cpu'
    }
    
    withName: 'PROCESS_SPLIT' {
        cpus = 4
        memory = '16 GB'
        time = '2h'
        queue = 'day-long-cpu'
        // Allow multiple jobs to run in parallel
        maxForks = 24
    }
    
    withName: 'COMBINE_RESULTS' {
        cpus = 1
        memory = '4 GB'
        time = '1h'
        queue = 'short-cpu'
    }
}

// Software management options (choose one)

// Option 1: Using conda/mamba
conda.enabled = true
process.conda = '/path/to/your/conda/env'  // or 'bioconda::seqkit'

// Option 2: Using singularity containers
// singularity.enabled = true
// process.container = 'biocontainers/seqkit:v2.3.1_cv1'

// Option 3: Using modules (if your HPC uses environment modules)
// process.module = ['seqkit/2.3.1', 'other-tools/version']

// Executor settings
executor {
    $slurm {
        // Maximum number of jobs to submit at once
        queueSize = 50
        
        // Time between job submissions
        submitRateLimit = '10 sec'
        
        // Job status polling interval
        pollInterval = '30 sec'
        
        // Maximum number of retry attempts
        retry = 3
    }
}

// Work directory (use fast local storage if available)
workDir = '/scratch/$USER/nextflow-work'

// Enable trace and timeline reports
trace {
    enabled = true
    file = 'pipeline_trace.txt'
}

timeline {
    enabled = true
    file = 'pipeline_timeline.html'
}

report {
    enabled = true
    file = 'pipeline_report.html'
}

dag {
    enabled = true
    file = 'pipeline_dag.svg'
}

// Profiles for different HPC environments
profiles {
    
    // Standard SLURM profile
    slurm_standard {
        process.executor = 'slurm'
        process.queue = 'day-long-cpu'
        process.memory = '4 GB'
        process.cpus = 1
        process.time = '1h'
    }
    
    // High memory nodes
    slurm_highmem {
        process.executor = 'slurm'
        process.queue = 'highmem'
        process.memory = '32 GB'
        process.cpus = 8
        process.time = '4h'
    }
    
    // GPU nodes (if needed for specialized processing)
    slurm_gpu {
        process.executor = 'slurm'
        process.queue = 'gpu'
        process.memory = '16 GB'
        process.cpus = 4
        process.time = '2h'
        process.clusterOptions = '--gres=gpu:1'
    }
    
    // Testing profile with minimal resources
    test {
        process.executor = 'slurm'
        process.queue = 'debug'
        process.memory = '2 GB'
        process.cpus = 1
        process.time = '15m'
        params.num_splits = 2
    }
}
